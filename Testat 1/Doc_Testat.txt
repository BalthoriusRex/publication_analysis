

Aufgabenstellung: Analyse und Visualisation von Publikationsdaten.
Aufgabe ist es einen bestehenden Datensatz als Datenquelle zu nutzen um mit dem Umgang von „größeren“ Datenmengen vertraut zu werden. Dabei sollen die Daten extrahiert, importiert, aufbereitet, analysiert und die Ergebnisse schließlich visualisiert werden.

Die Datenquelle ist dabei der Microsoft Academic Graph (MAG), welcher Informationen zu wissenschaftlichen Publikationen beinhaltet. Dazu zählen Metadaten wie Autoren, Institutionen, Konferenzen auf denen sie vorgestellt wurden oder aber auch Zitat-Beziehungen zwischen Autoren. 
Die Extraktion erfolgt dabei durch den Download der von Microsoft zur Verfügung gestellten Daten. 
Der Import wird mittels Hadoop realisiert. Dabei werden die Daten im Hadoop Distributed File System (kurz HDFS) gespeichert. Von dort aus kann mittels Spark auf die Daten zugegriffen und diesen aufgearbeitet werden.

Die Daten liegen dabei, sofern der Plan, auf einem HDFS eines entfernten Servers der Universität Leipzig auf den von unseren lokalen Arbeitssystemen aus zugegriffen wird. Erste Ansätzen zeigen dabei, dass es problematisch ist direkt vom lokalen System auf dem Remotesystem zu arbeiten, weshalb lokal der Quellcode mit einem Minimaldatensatz getestet wird und schließlich der Code als JAR verpackt auf den Remote transferiert wird um dort auf einer größeren Datenmenge zu arbeiten um die Fragestellung besser beantworten zu können. 
Unabhängig davon werden die Daten mittels der Java API von Spark geladen und für den weiteren Workflow aufgearbeitet. Dabei werden einzelne Zeilen der TXT-Dateien als Tupel verpackt um drauf zuzugreifen (Referenz: 
http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)

Die Aufarbeitung beinhaltet eine Reduktion der Daten, welche in entpackter Form in etwa 100GB beinhalten. Da es ein besonderer Aspekt der Analyse und Visualisierung sein soll, geographische Zusammenhänge zwischen Autoren zu erforschen, beinhaltet diese Reduktion insbesondere ein Entfernen von nicht gebrauchten Informationen zu den Orten, wodurch nur noch relevante Ortsangaben erhalten bleiben sollen. Neben diesen werden auch Informationen welche im Folgenden nicht gebraucht werden aus dem Arbeitsdatensatz entfernt um die Menge an Daten, welche vom Prozess verwaltet werden müssen, gering zu halten.

Für die Orte wird auf die normalisierten Affiliations zugegriffen. Neben den normalisierten Affiliations gibt es noch die originalen Einträge der Affiliations aus den Publikationen. Diese sind direkt aus den Publikationen extrahiert. Die normalisierte Form wurde von den original Einträgen abgeleitet und sollte einen Standard bieten, welcher es ermöglicht diese Daten automatisiert weiter zu verwenden.
Über die Güte dieser Informationen ist bisher keine Aussage zu treffen, ersten Beobachtungen nach lässt sich über diese normalisierte Form allerdings ein Bezug zu den Orten finden, welche zu den Affiliations gehören. 
Für das Mapping zwischen den Ortsnamen aus den Affiliations und Positionen auf einer Karte wird ein Geocodingdienst Namens OpenCageData (https://geocoder.opencagedata.com) genutzt. Diesem wird der normalisierten Affiliationstring übergeben und dann wird versucht an Hand dieses Namens einen in der OpenStreetMap Datenbank gespeicherten Ort ausfindig zu machen. Als Ergebnis wird nur der passenste Ort zurück geliefert. Das Ranking erfolgt dabei dienstintern und wird von uns nicht beeinflusst. 
Aus den erhaltenen Daten wird schließlich die Geoposition (in Longitude und Latitude) ermittelt, welche wiederum im letzten Schritt an OpenLayers und die OpenStreetMap für die Visualisierung überreicht wird.

Für die Analyse können verschiedene kleinere Queries genutzt werden um Besonderheiten oder allgemeine Zusammenhänge der Daten zu  betrachten.
Eine Reihe von exemplarischen Queries wäre wie folgt:
	„Aus welchen Orten wurden für den KDD Cup die meisten Paper ausgewählt?“
	„Aus welchen Jahren stammen die meisten der ausgewählten Paper?“
	„Auf welcher Konferenz wurden die meisten Paper aus Leipzig vorgestellt?“
	„Auf welcher Konferenz wurden die meisten Paper im Themenbereich Informatik 	vorgestellt?“
	„Auf welcher Konferenz wurden die meisten Paper in der kürzesten Zeit vorgestellt?“ 	[Stichwort Publikationsdichte]
	„Zu welchem Themengebiet liegen die meisten Publikationen vor?“
	„Welche Institution hat die meisten Paper veröffentlicht?“

	„Welche Autoren zitieren sich am meisten selbst?“
	„Wer ist der meistzitierte Autor?“
	„Welches ist das meistzitierte Paper?“

	„Welches Studiengebiet hat die meisten abgeleiteten Teilgebiete?“
	
Die Hauptaufgabe soll sein, über die Beziehung von Autoren (Co-Autoren) Aussagen über die Zusammenarbeit der zugehörigen Affiliations zu treffen.

-> Welche Affiliations haben wie oft mit welchen anderen Affiliations zusammengearbeitet?
-> Darstellung dieser Beziehung mittels OpenLayers

interessante Daten:
	PaperAuthorAffiliations.txt 		normalized affiliation name			-> geo daten
										author id							-> wer mit wem -> zugehörigkeit zu Affiliations -> welche mit welcher
										paper id

Offene Probleme:
Ersten Beobachtungen nach ist es nicht sicher, ob im MAG Co-Autorbeziehungen eingetragen sind. Um die Frage zu klären, ob mehrere Autoren pro Publikation abgespeichert sind oder nur der Hauptautor, ist eine Analyse mit der ganzen PaperAuthorAffiliationstabelle nötig, welche nach dem ersten Testat erfolgen wird.
Zudem bleibt die Frage nach der Reduktion der Daten. Im weiteren Verlauf wird entschieden werden müssen, welche Daten wirklich gebraucht werden (abhängig davon welche Queries im Endeffekt alle beantwortet werden sollen) und welche Daten durch den Reduktionsschritt außen vor gelassen werden können.
Bisherige Überlegungen beinhalten beispielsweise das Ignorieren der PaperURLs oder auch einzelner Spalten aus PaperAuthorAffiliations und Papers, welche den Umfang der Daten aufpusten ohne für uns von Relevanz zu sein.

	


